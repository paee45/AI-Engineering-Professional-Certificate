{"cells":[{"cell_type":"markdown","id":"01838a1b-a493-4e1e-9116-7e1ef3806da5","metadata":{},"outputs":[],"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n","    </a>\n","</p>\n","<h1 align=center><font size = 5>Convolutional Neral Network Simple example </font></h1> \n"]},{"cell_type":"markdown","id":"b456d68a-7a12-49f8-b6d3-42ac60ab3f1d","metadata":{},"outputs":[],"source":["\n","<h3>Objective for this Notebook<h3>    \n","<h5> 1. Learn Convolutional Neral Network</h5>\n","<h5> 2. Define Softmax , Criterion function, Optimizer and Train the  Model</h5>    \n","\n"]},{"cell_type":"markdown","id":"e86bc48d-3c80-4188-b3a8-677bf2721d63","metadata":{},"outputs":[],"source":["\n","# Table of Contents\n","In this lab, we will use a Convolutional Neral Networks to classify horizontal an vertical Lines \n","\n","<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n","<li><a href=\"#ref0\">Helper functions </a></li>\n","\n","<li><a href=\"#ref1\"> Prepare Data </a></li>\n","<li><a href=\"#ref2\">Convolutional Neral Network </a></li>\n","<li><a href=\"#ref3\">Define Softmax , Criterion function, Optimizer and Train the  Model</a></li>\n","<li><a href=\"#ref4\">Analyse Results</a></li>\n","\n","<br>\n","<p></p>\n","Estimated Time Needed: <strong>25 min</strong>\n","</div>\n","\n","<hr>\n"]},{"cell_type":"markdown","id":"e6638a38-f58b-4ffc-ac5f-4b0eee472afb","metadata":{},"outputs":[],"source":["<a id=\"ref0\"></a>\n","<h2 align=center>Helper functions </h2>\n"]},{"cell_type":"code","id":"bff688fe-583d-473e-a1fb-4eeb3b47766a","metadata":{},"outputs":[],"source":["import torch \nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\nimport matplotlib.pylab as plt\nimport numpy as np\nimport pandas as pd"]},{"cell_type":"code","id":"21b9fe7d-20fc-400b-af20-a85a9c5f324d","metadata":{},"outputs":[],"source":["torch.manual_seed(4)"]},{"cell_type":"markdown","id":"ce41c599-cff9-4ea8-a520-29e53e5787c0","metadata":{},"outputs":[],"source":["function to plot out the parameters of the Convolutional layers  \n"]},{"cell_type":"code","id":"de78dc80-d897-43ba-8038-12af4131e4f0","metadata":{},"outputs":[],"source":["def plot_channels(W):\n    #number of output channels \n    n_out=W.shape[0]\n    #number of input channels \n    n_in=W.shape[1]\n    w_min=W.min().item()\n    w_max=W.max().item()\n    fig, axes = plt.subplots(n_out,n_in)\n    fig.subplots_adjust(hspace = 0.1)\n    out_index=0\n    in_index=0\n    #plot outputs as rows inputs as columns \n    for ax in axes.flat:\n    \n        if in_index>n_in-1:\n            out_index=out_index+1\n            in_index=0\n              \n        ax.imshow(W[out_index,in_index,:,:], vmin=w_min, vmax=w_max, cmap='seismic')\n        ax.set_yticklabels([])\n        ax.set_xticklabels([])\n        in_index=in_index+1\n\n    plt.show()"]},{"cell_type":"markdown","id":"df7bd073-4725-4bea-b6d9-c319866c9844","metadata":{},"outputs":[],"source":["<code>show_data</code>: plot out data sample\n"]},{"cell_type":"code","id":"2c211435-0944-46cd-91ad-8bd5aa4e1b59","metadata":{},"outputs":[],"source":["def show_data(dataset,sample):\n\n    plt.imshow(dataset.x[sample,0,:,:].numpy(),cmap='gray')\n    plt.title('y='+str(dataset.y[sample].item()))\n    plt.show()"]},{"cell_type":"markdown","id":"437c1112-d8dc-4c4b-9117-70bd60449cf6","metadata":{},"outputs":[],"source":["create some toy data \n"]},{"cell_type":"code","id":"39631628-f8a5-4e5a-bc2d-748d3d0961b9","metadata":{},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\nclass Data(Dataset):\n    def __init__(self,N_images=100,offset=0,p=0.9, train=False):\n        \"\"\"\n        p:portability that pixel is wight  \n        N_images:number of images \n        offset:set a random vertical and horizontal offset images by a sample should be less than 3 \n        \"\"\"\n        if train==True:\n            np.random.seed(1)  \n        \n        #make images multiple of 3 \n        N_images=2*(N_images//2)\n        images=np.zeros((N_images,1,11,11))\n        start1=3\n        start2=1\n        self.y=torch.zeros(N_images).type(torch.long)\n\n        for n in range(N_images):\n            if offset>0:\n        \n                low=int(np.random.randint(low=start1, high=start1+offset, size=1))\n                high=int(np.random.randint(low=start2, high=start2+offset, size=1))\n            else:\n                low=4\n                high=1\n        \n            if n<=N_images//2:\n                self.y[n]=0\n                images[n,0,high:high+9,low:low+3]= np.random.binomial(1, p, (9,3))\n            elif  n>N_images//2:\n                self.y[n]=1\n                images[n,0,low:low+3,high:high+9] = np.random.binomial(1, p, (3,9))\n           \n        \n        \n        self.x=torch.from_numpy(images).type(torch.FloatTensor)\n        self.len=self.x.shape[0]\n        del(images)\n        np.random.seed(0)\n    def __getitem__(self,index):      \n        return self.x[index],self.y[index]\n    def __len__(self):\n        return self.len"]},{"cell_type":"markdown","id":"14269eb4-c61e-4861-8f40-c5b8cd6c81cd","metadata":{},"outputs":[],"source":["<code>plot_activation</code>: plot out the activations of the Convolutional layers  \n"]},{"cell_type":"code","id":"d020a705-a9b5-4321-bff5-f650035aaf4f","metadata":{},"outputs":[],"source":["def plot_activations(A,number_rows= 1,name=\"\"):\n    A=A[0,:,:,:].detach().numpy()\n    n_activations=A.shape[0]\n    \n    \n    print(n_activations)\n    A_min=A.min().item()\n    A_max=A.max().item()\n\n    if n_activations==1:\n\n        # Plot the image.\n        plt.imshow(A[0,:], vmin=A_min, vmax=A_max, cmap='seismic')\n\n    else:\n        fig, axes = plt.subplots(number_rows, n_activations//number_rows)\n        fig.subplots_adjust(hspace = 0.4)\n        for i,ax in enumerate(axes.flat):\n            if i< n_activations:\n                # Set the label for the sub-plot.\n                ax.set_xlabel( \"activation:{0}\".format(i+1))\n\n                # Plot the image.\n                ax.imshow(A[i,:], vmin=A_min, vmax=A_max, cmap='seismic')\n                ax.set_xticks([])\n                ax.set_yticks([])\n    plt.show()"]},{"cell_type":"markdown","id":"2ab3347c-abc9-4a60-a652-2d673f201526","metadata":{},"outputs":[],"source":["\n","Utility function for computing output of convolutions\n","takes a tuple of (h,w) and returns a tuple of (h,w)\n"]},{"cell_type":"code","id":"ddeed6a6-4fde-4b26-99d5-122082498aee","metadata":{},"outputs":[],"source":["\ndef conv_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n    #by Duane Nielsen\n    from math import floor\n    if type(kernel_size) is not tuple:\n        kernel_size = (kernel_size, kernel_size)\n    h = floor( ((h_w[0] + (2 * pad) - ( dilation * (kernel_size[0] - 1) ) - 1 )/ stride) + 1)\n    w = floor( ((h_w[1] + (2 * pad) - ( dilation * (kernel_size[1] - 1) ) - 1 )/ stride) + 1)\n    return h, w"]},{"cell_type":"markdown","id":"a175d2e0-72c7-4e42-8748-03dc2767de6c","metadata":{},"outputs":[],"source":["<a id=\"ref1\"></a>\n","<h2 align=center>Prepare Data </h2> \n"]},{"cell_type":"markdown","id":"a93b3754-b5bd-466b-9ebe-b3ecbac1f998","metadata":{},"outputs":[],"source":["Load the training dataset with 10000 samples \n"]},{"cell_type":"code","id":"83136c65-9728-49ec-bf09-433c01662e89","metadata":{},"outputs":[],"source":["N_images=10000\ntrain_dataset=Data(N_images=N_images)"]},{"cell_type":"markdown","id":"a5df1d3b-b8ea-4be2-af50-6515bc60a567","metadata":{},"outputs":[],"source":["Load the testing dataset\n"]},{"cell_type":"code","id":"cb92e5b5-27e3-4a4d-9fb5-56bfc3da2120","metadata":{},"outputs":[],"source":["validation_dataset=Data(N_images=1000,train=False)\nvalidation_dataset"]},{"cell_type":"markdown","id":"4dbf0b1b-d0c0-4c73-956f-8bc4d276f21e","metadata":{},"outputs":[],"source":["we can see the data type is long \n"]},{"cell_type":"markdown","id":"cc41efad-61b3-4c4c-bc18-a8c41a5c30cb","metadata":{},"outputs":[],"source":["### Data Visualization \n"]},{"cell_type":"markdown","id":"841f9920-b1df-494e-9560-c09cfdae4552","metadata":{},"outputs":[],"source":["Each element in the rectangular  tensor corresponds to a number representing a pixel intensity  as demonstrated by  the following image.\n"]},{"cell_type":"markdown","id":"9f1ce4ff-e1e5-4d6b-9f6e-254be5995a33","metadata":{},"outputs":[],"source":["We can print out the third label \n"]},{"cell_type":"code","id":"06a704d5-a8d4-4493-b02b-cb33efb405d4","metadata":{},"outputs":[],"source":["show_data(train_dataset,0)"]},{"cell_type":"code","id":"5a05eb93-f0b7-4577-be77-8023c0dc4141","metadata":{},"outputs":[],"source":["show_data(train_dataset,N_images//2+2)"]},{"cell_type":"markdown","id":"f13d0db9-4908-4df4-96ff-57f6730729c7","metadata":{},"outputs":[],"source":["we can plot the 3rd  sample \n"]},{"cell_type":"markdown","id":"0846d950-c49d-4a23-829d-ec2975a0e72d","metadata":{},"outputs":[],"source":["<a id=\"ref3\"></a>\n","### Build a Convolutional Neral Network Class \n"]},{"cell_type":"markdown","id":"b8cdd91d-d592-44be-b11a-26cd5635f9c6","metadata":{},"outputs":[],"source":["The input image is 11 x11, the following will change the size of the activations:\n","<ul>\n","<il>convolutional layer</il> \n","</ul>\n","<ul>\n","<il>max pooling layer</il> \n","</ul>\n","<ul>\n","<il>convolutional layer </il>\n","</ul>\n","<ul>\n","<il>max pooling layer </il>\n","</ul>\n","\n","with the following parameters <code>kernel_size</code>, <code>stride</code> and <code> pad</code>.\n","We use the following  lines of code to change the image before we get tot he fully connected layer \n"]},{"cell_type":"code","id":"d86e957c-52cf-4ff4-bd6d-5a35e5d1650b","metadata":{},"outputs":[],"source":["out=conv_output_shape((11,11), kernel_size=2, stride=1, pad=0, dilation=1)\nprint(out)\nout1=conv_output_shape(out, kernel_size=2, stride=1, pad=0, dilation=1)\nprint(out1)\nout2=conv_output_shape(out1, kernel_size=2, stride=1, pad=0, dilation=1)\nprint(out2)\n\nout3=conv_output_shape(out2, kernel_size=2, stride=1, pad=0, dilation=1)\nprint(out3)"]},{"cell_type":"markdown","id":"8c088b8a-4832-4025-8d45-62c2b66d2dfe","metadata":{},"outputs":[],"source":["Build a Convolutional Network class with two Convolutional layers and one fully connected layer. Pre-determine the size of the final output matrix. The parameters in the constructor are the number of output channels for the first and second layer.\n"]},{"cell_type":"code","id":"7687da42-eb49-4252-9bbe-8d9e6d068bcf","metadata":{},"outputs":[],"source":["class CNN(nn.Module):\n    def __init__(self,out_1=2,out_2=1):\n        \n        super(CNN,self).__init__()\n        #first Convolutional layers \n        self.cnn1=nn.Conv2d(in_channels=1,out_channels=out_1,kernel_size=2,padding=0)\n        self.maxpool1=nn.MaxPool2d(kernel_size=2 ,stride=1)\n\n        #second Convolutional layers\n        self.cnn2=nn.Conv2d(in_channels=out_1,out_channels=out_2,kernel_size=2,stride=1,padding=0)\n        self.maxpool2=nn.MaxPool2d(kernel_size=2 ,stride=1)\n        #max pooling \n\n        #fully connected layer \n        self.fc1=nn.Linear(out_2*7*7,2)\n        \n    def forward(self,x):\n        #first Convolutional layers\n        x=self.cnn1(x)\n        #activation function \n        x=torch.relu(x)\n        #max pooling \n        x=self.maxpool1(x)\n        #first Convolutional layers\n        x=self.cnn2(x)\n        #activation function\n        x=torch.relu(x)\n        #max pooling\n        x=self.maxpool2(x)\n        #flatten output \n        x=x.view(x.size(0),-1)\n        #fully connected layer\n        x=self.fc1(x)\n        return x\n    \n    def activations(self,x):\n        #outputs activation this is not necessary just for fun \n        z1=self.cnn1(x)\n        a1=torch.relu(z1)\n        out=self.maxpool1(a1)\n        \n        z2=self.cnn2(out)\n        a2=torch.relu(z2)\n        out=self.maxpool2(a2)\n        out=out.view(out.size(0),-1)\n        return z1,a1,z2,a2,out        "]},{"cell_type":"markdown","id":"d13244c3-f1ec-4db3-9c1f-f1d87815d3da","metadata":{},"outputs":[],"source":["<a id=\"ref3\"></a>\n","<h2> Define the Convolutional Neral Network Classifier , Criterion function, Optimizer and Train the  Model  </h2> \n"]},{"cell_type":"markdown","id":"eacfea7a-407e-4d04-a654-7d3a4d8f4062","metadata":{},"outputs":[],"source":["There are 2 output channels for the first layer, and 1 outputs channel for the second layer \n"]},{"cell_type":"code","id":"b395fb0c-99fb-41af-a8f2-bae173bb11ab","metadata":{},"outputs":[],"source":["model=CNN(2,1)"]},{"cell_type":"markdown","id":"b91d4543-6cbb-4605-9373-32a772f31116","metadata":{},"outputs":[],"source":["we can see the model parameters with the object \n"]},{"cell_type":"code","id":"ba12e3fb-513b-462c-9584-8dd56e2d4979","metadata":{},"outputs":[],"source":["model"]},{"cell_type":"markdown","id":"daa175f5-52a8-4a9d-8533-0fb442b7c0f0","metadata":{},"outputs":[],"source":["Plot the model parameters for the kernels before training the kernels. The kernels are initialized randomly.\n"]},{"cell_type":"code","id":"71f7fc71-8787-452f-a540-a8bb07467f7b","metadata":{},"outputs":[],"source":["\nplot_channels(model.state_dict()['cnn1.weight'])\n"]},{"cell_type":"markdown","id":"c22d08a9-c96f-457c-9417-efd506ac4439","metadata":{},"outputs":[],"source":["Loss function \n"]},{"cell_type":"code","id":"d3820d64-fba7-48bb-b13d-e9df12e29594","metadata":{},"outputs":[],"source":["plot_channels(model.state_dict()['cnn2.weight'])"]},{"cell_type":"markdown","id":"cfee82a2-4adc-4450-a355-d0cc99651562","metadata":{},"outputs":[],"source":["Define the loss function \n"]},{"cell_type":"code","id":"c5391fbe-a02d-4b1e-8c5d-2b5b3b7efb3f","metadata":{},"outputs":[],"source":["criterion=nn.CrossEntropyLoss()"]},{"cell_type":"markdown","id":"b780bdbe-df1b-4488-a472-7bbe72103010","metadata":{},"outputs":[],"source":[" optimizer class \n"]},{"cell_type":"code","id":"4bae72c7-2177-4e4a-bde1-42f08e45b4db","metadata":{},"outputs":[],"source":["learning_rate=0.001\n\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"]},{"cell_type":"markdown","id":"f72e377d-1454-42dd-a83c-6153111015ca","metadata":{},"outputs":[],"source":["Define the optimizer class \n"]},{"cell_type":"code","id":"dd23f276-4a95-4eee-893a-07be94887339","metadata":{},"outputs":[],"source":["\ntrain_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=10)\nvalidation_loader=torch.utils.data.DataLoader(dataset=validation_dataset,batch_size=20)"]},{"cell_type":"markdown","id":"921a5d0e-9261-4165-88f4-eda4b8c54f58","metadata":{},"outputs":[],"source":["Train the model and determine validation accuracy technically test accuracy **(This may take a long time)**\n"]},{"cell_type":"code","id":"774a9ac0-0f5a-46f7-b10d-fce0dacbde04","metadata":{},"outputs":[],"source":["n_epochs=10\ncost_list=[]\naccuracy_list=[]\nN_test=len(validation_dataset)\ncost=0\n#n_epochs\nfor epoch in range(n_epochs):\n    cost=0    \n    for x, y in train_loader:\n      \n\n        #clear gradient \n        optimizer.zero_grad()\n        #make a prediction \n        z=model(x)\n        # calculate loss \n        loss=criterion(z,y)\n        # calculate gradients of parameters \n        loss.backward()\n        # update parameters \n        optimizer.step()\n        cost+=loss.item()\n    cost_list.append(cost)\n        \n        \n    correct=0\n    #perform a prediction on the validation  data  \n    for x_test, y_test in validation_loader:\n\n        z=model(x_test)\n        _,yhat=torch.max(z.data,1)\n\n        correct+=(yhat==y_test).sum().item()\n        \n\n    accuracy=correct/N_test\n\n    accuracy_list.append(accuracy)\n    \n\n"]},{"cell_type":"markdown","id":"42c758d2-b853-42c1-b00e-b5389955886c","metadata":{},"outputs":[],"source":["#### <a id=\"ref3\"></a>\n","<h2 align=center>Analyse Results</h2> \n"]},{"cell_type":"markdown","id":"7a2da874-55c5-4f9c-8ff2-208aadc59437","metadata":{},"outputs":[],"source":["Plot the loss and accuracy on the validation data:\n"]},{"cell_type":"code","id":"2288fdbd-7cae-4b72-8fd8-366cfd11d2c5","metadata":{},"outputs":[],"source":["fig, ax1 = plt.subplots()\ncolor = 'tab:red'\nax1.plot(cost_list,color=color)\nax1.set_xlabel('epoch',color=color)\nax1.set_ylabel('total loss',color=color)\nax1.tick_params(axis='y', color=color)\n    \nax2 = ax1.twinx()  \ncolor = 'tab:blue'\nax2.set_ylabel('accuracy', color=color)  \nax2.plot( accuracy_list, color=color)\nax2.tick_params(axis='y', labelcolor=color)\nfig.tight_layout()"]},{"cell_type":"markdown","id":"f7aa573a-b1cc-482a-85a1-6f7be112139f","metadata":{},"outputs":[],"source":["View the results of the parameters for the Convolutional layers \n"]},{"cell_type":"code","id":"a77feb11-14b7-4b21-b2cf-b6e28ac49156","metadata":{},"outputs":[],"source":["model.state_dict()['cnn1.weight']"]},{"cell_type":"code","id":"abf41381-c048-42f7-8854-902bef5d07fc","metadata":{},"outputs":[],"source":["plot_channels(model.state_dict()['cnn1.weight'])"]},{"cell_type":"code","id":"512c8b0a-a785-4755-8eac-2ebde57cd491","metadata":{},"outputs":[],"source":["model.state_dict()['cnn1.weight']"]},{"cell_type":"code","id":"d7445c0c-f13a-40f0-aac3-2f4393281996","metadata":{},"outputs":[],"source":["plot_channels(model.state_dict()['cnn2.weight'])"]},{"cell_type":"markdown","id":"810af4ea-c66e-4856-98b8-a341f8f0172e","metadata":{},"outputs":[],"source":["Consider the following sample \n"]},{"cell_type":"code","id":"ad2b35d6-4086-4e99-94f8-3041233c86b8","metadata":{},"outputs":[],"source":["show_data(train_dataset,N_images//2+2)"]},{"cell_type":"markdown","id":"d5e7dc02-27fc-4995-8287-14c000cc8d66","metadata":{},"outputs":[],"source":["Determine the activations \n"]},{"cell_type":"code","id":"e7101873-1d74-4962-adaa-839da5ca9d64","metadata":{},"outputs":[],"source":["out=model.activations(train_dataset[N_images//2+2][0].view(1,1,11,11))\nout=model.activations(train_dataset[0][0].view(1,1,11,11))"]},{"cell_type":"markdown","id":"ab5d1b10-73c5-4e21-8002-420768cb69f0","metadata":{},"outputs":[],"source":["Plot them out\n"]},{"cell_type":"code","id":"3c4e93db-ad0b-4469-8f86-ae2787561bdc","metadata":{},"outputs":[],"source":["plot_activations(out[0],number_rows=1,name=\" feature map\")\nplt.show()\n"]},{"cell_type":"code","id":"1523fa3d-82d2-47e3-aff7-e729017c31a1","metadata":{},"outputs":[],"source":["plot_activations(out[2],number_rows=1,name=\"2nd feature map\")\nplt.show()"]},{"cell_type":"code","id":"9fafbb19-7ce2-46eb-9806-dd3ee7d01176","metadata":{},"outputs":[],"source":["plot_activations(out[3],number_rows=1,name=\"first feature map\")\nplt.show()"]},{"cell_type":"markdown","id":"8a340bff-2015-4f96-a929-043a7e10d5a6","metadata":{},"outputs":[],"source":["we save the output of the activation after flattening  \n"]},{"cell_type":"code","id":"10cbd6ec-2550-418c-a12d-6458d14d5649","metadata":{},"outputs":[],"source":["out1=out[4][0].detach().numpy()"]},{"cell_type":"markdown","id":"c21e9852-1d0c-4e55-97ae-3a1dd6908310","metadata":{},"outputs":[],"source":["we can do the same for a sample  where y=0 \n"]},{"cell_type":"code","id":"1de6acc2-c27f-4a31-b3b4-1dce12aba4c1","metadata":{},"outputs":[],"source":["out0=model.activations(train_dataset[100][0].view(1,1,11,11))[4][0].detach().numpy()\nout0"]},{"cell_type":"code","id":"4e362dc6-92d4-4ad8-acfa-7a46fec5889f","metadata":{},"outputs":[],"source":["plt.subplot(2, 1, 1)\nplt.plot( out1, 'b')\nplt.title('Flatted Activation Values  ')\nplt.ylabel('Activation')\nplt.xlabel('index')\nplt.subplot(2, 1, 2)\nplt.plot(out0, 'r')\nplt.xlabel('index')\nplt.ylabel('Activation')"]},{"cell_type":"markdown","id":"17a16501-0ae6-4c87-a8eb-c3bc4bdb4e4a","metadata":{},"outputs":[],"source":["\n","\n","<a href=\"https://dataplatform.cloud.ibm.com/registration/stepone?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork&context=cpdaas&apps=data_science_experience%2Cwatson_machine_learning\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork/Template/module%201/images/Watson_Studio.png\"></a>\n","\n"]},{"cell_type":"markdown","id":"b4b6a6f8-d115-4349-b26f-7f5f93511c29","metadata":{},"outputs":[],"source":["### About the Authors:  \n","[Joseph Santarcangelo]( https://www.linkedin.com/in/joseph-s-50398b136/) has a PhD in Electrical Engineering. His research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. \n","\n","Other contributors: [Michelle Carey](  https://www.linkedin.com/in/michelleccarey/) \n"]},{"cell_type":"markdown","id":"c845f903-6b94-4076-82e7-e392386f18f4","metadata":{},"outputs":[],"source":["<!--\n","## Change Log\n","\n","|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n","|---|---|---|---|\n","| 2020-09-23  | 2.0  | Srishti  |  Migrated Lab to Markdown and added to course repo in GitLab |\n","\n","\n","\n","<hr>\n","-->\n","\n","## <h3 align=\"center\"> &#169; IBM Corporation. All rights reserved. <h3/>\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""}},"nbformat":4,"nbformat_minor":4}